{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Performance Benchmarks: rapid_textrank vs pytextrank\n",
    "\n",
    "This notebook compares the performance of `rapid_textrank` (Rust) against `pytextrank` (Python).\n",
    "\n",
    "**What we'll measure:**\n",
    "1. End-to-end extraction time (including tokenization)\n",
    "2. Extraction-only time (both using spaCy tokens)\n",
    "3. Batch processing performance\n",
    "4. Quality comparison of extracted keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n%pip install -q rapid_textrank pytextrank spacy matplotlib pandas\nimport sys\n!{sys.executable} -m spacy download en_core_web_sm -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methodology",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We'll run benchmarks on three text sizes:\n",
    "- **Small** (~20 words): Quick extraction\n",
    "- **Medium** (~120 words): Typical document\n",
    "- **Large** (~1000 words): Long-form content\n",
    "\n",
    "Each benchmark runs multiple iterations to get stable timing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-texts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts of varying sizes\n",
    "TEXTS = {\n",
    "    \"small\": \"\"\"\n",
    "        Machine learning is a subset of artificial intelligence.\n",
    "        Deep learning uses neural networks with many layers.\n",
    "    \"\"\",\n",
    "\n",
    "    \"medium\": \"\"\"\n",
    "        Natural language processing (NLP) is a field of artificial intelligence\n",
    "        that focuses on the interaction between computers and humans through\n",
    "        natural language. The ultimate goal of NLP is to enable computers to\n",
    "        understand, interpret, and generate human language in a valuable way.\n",
    "\n",
    "        Machine learning approaches have transformed NLP in recent years.\n",
    "        Deep learning models, particularly transformers, have achieved\n",
    "        state-of-the-art results on many NLP tasks including translation,\n",
    "        summarization, and question answering.\n",
    "\n",
    "        Key applications include sentiment analysis, named entity recognition,\n",
    "        machine translation, and text classification. These technologies\n",
    "        power virtual assistants, search engines, and content recommendation\n",
    "        systems used by millions of people daily.\n",
    "    \"\"\",\n",
    "\n",
    "    \"large\": \"\"\"\n",
    "        Artificial intelligence has evolved dramatically since its inception in\n",
    "        the mid-20th century. Early AI systems relied on symbolic reasoning and\n",
    "        expert systems, where human knowledge was manually encoded into rules.\n",
    "\n",
    "        The machine learning revolution changed everything. Instead of explicit\n",
    "        programming, systems learn patterns from data. Supervised learning uses\n",
    "        labeled examples, unsupervised learning finds hidden structures, and\n",
    "        reinforcement learning optimizes through trial and error.\n",
    "\n",
    "        Deep learning, powered by neural networks with multiple layers, has\n",
    "        achieved remarkable success. Convolutional neural networks excel at\n",
    "        image recognition. Recurrent neural networks and transformers handle\n",
    "        sequential data like text and speech. Generative adversarial networks\n",
    "        create realistic synthetic content.\n",
    "\n",
    "        Natural language processing has been transformed by these advances.\n",
    "        Word embeddings capture semantic relationships. Attention mechanisms\n",
    "        allow models to focus on relevant context. Large language models\n",
    "        demonstrate emergent capabilities in reasoning and generation.\n",
    "\n",
    "        Computer vision applications include object detection, facial recognition,\n",
    "        medical image analysis, and autonomous vehicle perception. These systems\n",
    "        process visual information with superhuman accuracy in many domains.\n",
    "\n",
    "        The ethical implications of AI are significant. Bias in training data\n",
    "        can lead to unfair outcomes. Privacy concerns arise from data collection.\n",
    "        Job displacement affects workers across industries. Regulation and\n",
    "        governance frameworks are being developed worldwide.\n",
    "\n",
    "        Future directions include neuromorphic computing, quantum machine learning,\n",
    "        and artificial general intelligence. Researchers continue to push\n",
    "        boundaries while addressing safety and alignment challenges.\n",
    "    \"\"\" * 3  # ~1000 words\n",
    "}\n",
    "\n",
    "# Print word counts\n",
    "for name, text in TEXTS.items():\n",
    "    word_count = len(text.split())\n",
    "    print(f\"{name}: ~{word_count} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark1-header",
   "metadata": {},
   "source": [
    "## Benchmark 1: End-to-End (Simple Text API)\n",
    "\n",
    "This benchmark measures the total time from raw text to extracted keywords.\n",
    "\n",
    "- **rapid_textrank**: Uses built-in Rust tokenizer\n",
    "- **pytextrank**: Uses spaCy for tokenization and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_rapid_textrank(text: str, runs: int = 10) -> dict:\n",
    "    \"\"\"Benchmark rapid_textrank.\"\"\"\n",
    "    from rapid_textrank import BaseTextRank\n",
    "\n",
    "    extractor = BaseTextRank(top_n=10, language=\"en\")\n",
    "\n",
    "    # Warmup\n",
    "    extractor.extract_keywords(text)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        result = extractor.extract_keywords(text)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed * 1000)  # Convert to ms\n",
    "\n",
    "    return {\n",
    "        \"min\": min(times),\n",
    "        \"mean\": statistics.mean(times),\n",
    "        \"median\": statistics.median(times),\n",
    "        \"std\": statistics.stdev(times) if len(times) > 1 else 0,\n",
    "        \"phrases\": len(result.phrases)\n",
    "    }\n",
    "\n",
    "\n",
    "def benchmark_pytextrank(text: str, runs: int = 10) -> dict:\n",
    "    \"\"\"Benchmark pytextrank with spaCy.\"\"\"\n",
    "    import spacy\n",
    "    import pytextrank\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"textrank\")\n",
    "\n",
    "    # Warmup\n",
    "    doc = nlp(text)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        doc = nlp(text)\n",
    "        phrases = list(doc._.phrases[:10])\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed * 1000)\n",
    "\n",
    "    return {\n",
    "        \"min\": min(times),\n",
    "        \"mean\": statistics.mean(times),\n",
    "        \"median\": statistics.median(times),\n",
    "        \"std\": statistics.stdev(times) if len(times) > 1 else 0,\n",
    "        \"phrases\": len(phrases)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmark1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run end-to-end benchmarks\n",
    "results = {}\n",
    "\n",
    "print(\"Running end-to-end benchmarks...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for size, text in TEXTS.items():\n",
    "    word_count = len(text.split())\n",
    "    print(f\"\\n{size.upper()} TEXT (~{word_count} words)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Benchmark rapid_textrank\n",
    "    rust_results = benchmark_rapid_textrank(text)\n",
    "    print(f\"rapid_textrank:  {rust_results['mean']:>8.2f} ms (±{rust_results['std']:.2f})\")\n",
    "\n",
    "    # Benchmark pytextrank\n",
    "    py_results = benchmark_pytextrank(text)\n",
    "    print(f\"pytextrank:      {py_results['mean']:>8.2f} ms (±{py_results['std']:.2f})\")\n",
    "\n",
    "    speedup = py_results['mean'] / rust_results['mean']\n",
    "    print(f\"Speedup:         {speedup:>8.1f}x faster\")\n",
    "\n",
    "    results[size] = {\n",
    "        \"rapid_textrank\": rust_results,\n",
    "        \"pytextrank\": py_results,\n",
    "        \"speedup\": speedup\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark1-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sizes = list(results.keys())\n",
    "rapid_times = [results[s]['rapid_textrank']['mean'] for s in sizes]\n",
    "py_times = [results[s]['pytextrank']['mean'] for s in sizes]\n",
    "speedups = [results[s]['speedup'] for s in sizes]\n",
    "\n",
    "# Bar chart comparing times\n",
    "x = np.arange(len(sizes))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, rapid_times, width, label='rapid_textrank', color='steelblue')\n",
    "bars2 = axes[0].bar(x + width/2, py_times, width, label='pytextrank', color='coral')\n",
    "\n",
    "axes[0].set_ylabel('Time (ms)', fontsize=12)\n",
    "axes[0].set_xlabel('Text Size', fontsize=12)\n",
    "axes[0].set_title('End-to-End Extraction Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([s.capitalize() for s in sizes])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, rapid_times):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}', \n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "for bar, val in zip(bars2, py_times):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}', \n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Speedup chart\n",
    "bars3 = axes[1].bar(x, speedups, color='forestgreen', edgecolor='darkgreen')\n",
    "axes[1].set_ylabel('Speedup Factor (x)', fontsize=12)\n",
    "axes[1].set_xlabel('Text Size', fontsize=12)\n",
    "axes[1].set_title('Speedup: rapid_textrank vs pytextrank', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([s.capitalize() for s in sizes])\n",
    "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars3, speedups):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}x', \n",
    "                 ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark2-header",
   "metadata": {},
   "source": [
    "## Benchmark 2: Extraction-Only (Apples-to-Apples)\n",
    "\n",
    "The previous benchmark includes tokenization, which differs between the two libraries. For a fairer comparison, let's measure extraction-only time when both use the same spaCy tokenization.\n",
    "\n",
    "This tests the **JSON API** of rapid_textrank, which accepts pre-tokenized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spacy-to-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def prepare_spacy_tokens_for_json(doc):\n",
    "    \"\"\"Convert spaCy Doc to rapid_textrank JSON format.\"\"\"\n",
    "    tokens = []\n",
    "    for sent_idx, sent in enumerate(doc.sents):\n",
    "        for token in sent:\n",
    "            tokens.append({\n",
    "                \"text\": token.text,\n",
    "                \"lemma\": token.lemma_,\n",
    "                \"pos\": token.pos_,\n",
    "                \"start\": token.idx,\n",
    "                \"end\": token.idx + len(token.text),\n",
    "                \"sentence_idx\": sent_idx,\n",
    "                \"token_idx\": token.i,\n",
    "                \"is_stopword\": token.is_stop\n",
    "            })\n",
    "    return tokens\n",
    "\n",
    "# Pre-tokenize all texts with spaCy\n",
    "tokenized_texts = {}\n",
    "for name, text in TEXTS.items():\n",
    "    doc = nlp(text)\n",
    "    tokenized_texts[name] = {\n",
    "        \"doc\": doc,\n",
    "        \"tokens\": prepare_spacy_tokens_for_json(doc)\n",
    "    }\n",
    "    print(f\"{name}: {len(tokenized_texts[name]['tokens'])} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraction-only-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapid_textrank import extract_from_json\n",
    "import pytextrank\n",
    "\n",
    "def benchmark_rapid_extraction_only(tokens, runs=10):\n",
    "    \"\"\"Benchmark rapid_textrank extraction (no tokenization).\"\"\"\n",
    "    doc = {\"tokens\": tokens, \"config\": {\"top_n\": 10}}\n",
    "    json_input = json.dumps(doc)\n",
    "    \n",
    "    # Warmup\n",
    "    extract_from_json(json_input)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        result_json = extract_from_json(json_input)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed * 1000)\n",
    "    \n",
    "    return {\n",
    "        \"min\": min(times),\n",
    "        \"mean\": statistics.mean(times),\n",
    "        \"std\": statistics.stdev(times) if len(times) > 1 else 0\n",
    "    }\n",
    "\n",
    "def benchmark_pytextrank_extraction_only(doc, runs=10):\n",
    "    \"\"\"Benchmark pytextrank extraction (reusing tokenized doc).\"\"\"\n",
    "    # Create a new nlp with just textrank (no other pipes)\n",
    "    nlp_extract = spacy.load(\"en_core_web_sm\")\n",
    "    nlp_extract.add_pipe(\"textrank\")\n",
    "    \n",
    "    # Get the textrank component\n",
    "    textrank = nlp_extract.get_pipe(\"textrank\")\n",
    "    \n",
    "    # Warmup - need to process through the full pipeline once\n",
    "    test_doc = nlp_extract(doc.text)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        # Re-process to measure textrank time\n",
    "        # Note: pytextrank doesn't separate tokenization from extraction well\n",
    "        # so we measure the textrank pipe specifically\n",
    "        fresh_doc = nlp_extract.make_doc(doc.text)\n",
    "        for name, proc in nlp_extract.pipeline:\n",
    "            if name == \"textrank\":\n",
    "                start = time.perf_counter()\n",
    "                fresh_doc = proc(fresh_doc)\n",
    "                elapsed = time.perf_counter() - start\n",
    "                times.append(elapsed * 1000)\n",
    "            else:\n",
    "                fresh_doc = proc(fresh_doc)\n",
    "    \n",
    "    return {\n",
    "        \"min\": min(times),\n",
    "        \"mean\": statistics.mean(times),\n",
    "        \"std\": statistics.stdev(times) if len(times) > 1 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmark2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run extraction-only benchmarks\n",
    "extraction_results = {}\n",
    "\n",
    "print(\"Running extraction-only benchmarks (same spaCy tokenization)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for size in TEXTS.keys():\n",
    "    tokens = tokenized_texts[size]['tokens']\n",
    "    doc = tokenized_texts[size]['doc']\n",
    "    \n",
    "    print(f\"\\n{size.upper()} TEXT ({len(tokens)} tokens)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    rapid_res = benchmark_rapid_extraction_only(tokens)\n",
    "    print(f\"rapid_textrank (JSON API):  {rapid_res['mean']:>8.3f} ms (±{rapid_res['std']:.3f})\")\n",
    "    \n",
    "    py_res = benchmark_pytextrank_extraction_only(doc)\n",
    "    print(f\"pytextrank (extraction):    {py_res['mean']:>8.3f} ms (±{py_res['std']:.3f})\")\n",
    "    \n",
    "    speedup = py_res['mean'] / rapid_res['mean'] if rapid_res['mean'] > 0 else float('inf')\n",
    "    print(f\"Speedup:                    {speedup:>8.1f}x faster\")\n",
    "    \n",
    "    extraction_results[size] = {\n",
    "        \"rapid_textrank\": rapid_res,\n",
    "        \"pytextrank\": py_res,\n",
    "        \"speedup\": speedup\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark2-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize extraction-only results\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sizes = list(extraction_results.keys())\n",
    "x = np.arange(len(sizes))\n",
    "width = 0.35\n",
    "\n",
    "rapid_times = [extraction_results[s]['rapid_textrank']['mean'] for s in sizes]\n",
    "py_times = [extraction_results[s]['pytextrank']['mean'] for s in sizes]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, rapid_times, width, label='rapid_textrank (JSON API)', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, py_times, width, label='pytextrank', color='coral')\n",
    "\n",
    "ax.set_ylabel('Time (ms)', fontsize=12)\n",
    "ax.set_xlabel('Text Size', fontsize=12)\n",
    "ax.set_title('Extraction-Only Time (Same Tokenization)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([s.capitalize() for s in sizes])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add speedup annotations\n",
    "for i, size in enumerate(sizes):\n",
    "    speedup = extraction_results[size]['speedup']\n",
    "    max_height = max(rapid_times[i], py_times[i])\n",
    "    ax.annotate(f'{speedup:.1f}x', xy=(i, max_height), xytext=(i, max_height + 0.1),\n",
    "                ha='center', fontsize=10, fontweight='bold', color='forestgreen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark3-header",
   "metadata": {},
   "source": [
    "## Benchmark 3: Batch Processing\n",
    "\n",
    "For processing many documents, rapid_textrank's `extract_batch_from_json` function can process multiple documents efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapid_textrank import extract_batch_from_json, BaseTextRank\n",
    "\n",
    "# Create 100 documents of varying sizes\n",
    "num_docs = 100\n",
    "batch_texts = [TEXTS['medium']] * num_docs  # Use medium text\n",
    "\n",
    "# Pre-tokenize for JSON API\n",
    "batch_tokens = []\n",
    "for text in batch_texts:\n",
    "    doc = nlp(text)\n",
    "    tokens = prepare_spacy_tokens_for_json(doc)\n",
    "    batch_tokens.append({\"tokens\": tokens, \"config\": {\"top_n\": 10}})\n",
    "\n",
    "print(f\"Prepared {num_docs} documents for batch processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-batch-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_rapid_batch(docs, runs=5):\n",
    "    \"\"\"Benchmark rapid_textrank batch processing.\"\"\"\n",
    "    json_input = json.dumps(docs)\n",
    "    \n",
    "    # Warmup\n",
    "    extract_batch_from_json(json_input)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        results = extract_batch_from_json(json_input)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed * 1000)\n",
    "    \n",
    "    return statistics.mean(times), statistics.stdev(times)\n",
    "\n",
    "def benchmark_pytextrank_batch(texts, runs=5):\n",
    "    \"\"\"Benchmark pytextrank on batch (sequential processing).\"\"\"\n",
    "    nlp_py = spacy.load(\"en_core_web_sm\")\n",
    "    nlp_py.add_pipe(\"textrank\")\n",
    "    \n",
    "    # Warmup\n",
    "    for text in texts[:5]:\n",
    "        doc = nlp_py(text)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        for text in texts:\n",
    "            doc = nlp_py(text)\n",
    "            phrases = list(doc._.phrases[:10])\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed * 1000)\n",
    "    \n",
    "    return statistics.mean(times), statistics.stdev(times)\n",
    "\n",
    "print(f\"Benchmarking batch processing ({num_docs} documents)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rapid_mean, rapid_std = benchmark_rapid_batch(batch_tokens)\n",
    "print(f\"rapid_textrank (batch JSON): {rapid_mean:>10.2f} ms (±{rapid_std:.2f})\")\n",
    "print(f\"  Per document:              {rapid_mean/num_docs:>10.3f} ms\")\n",
    "\n",
    "py_mean, py_std = benchmark_pytextrank_batch(batch_texts)\n",
    "print(f\"pytextrank (sequential):     {py_mean:>10.2f} ms (±{py_std:.2f})\")\n",
    "print(f\"  Per document:              {py_mean/num_docs:>10.3f} ms\")\n",
    "\n",
    "speedup = py_mean / rapid_mean\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-fast",
   "metadata": {},
   "source": [
    "## Why is rapid_textrank Faster?\n",
    "\n",
    "The performance advantage comes from several factors in the Rust implementation:\n",
    "\n",
    "### 1. CSR Graph Format\n",
    "The co-occurrence graph uses **Compressed Sparse Row (CSR)** format, enabling cache-friendly memory access during PageRank iteration.\n",
    "\n",
    "### 2. String Interning\n",
    "Repeated words share a single allocation via `StringPool`, reducing memory usage 10-100x for typical documents.\n",
    "\n",
    "### 3. Parallel Processing\n",
    "Rayon provides data parallelism in internal graph construction without explicit thread management.\n",
    "\n",
    "### 4. Link-Time Optimization (LTO)\n",
    "Release builds use full LTO with single codegen unit for maximum inlining.\n",
    "\n",
    "### 5. Rust Core\n",
    "Most computation happens in compiled Rust code, minimizing Python interpreter overhead.\n",
    "\n",
    "### 6. FxHash\n",
    "Fast non-cryptographic hashing for internal hash maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-header",
   "metadata": {},
   "source": [
    "## Quality Comparison\n",
    "\n",
    "Speed is only useful if results are comparable. Let's compare the top keywords from both libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords with both libraries\n",
    "text = TEXTS['medium']\n",
    "\n",
    "# rapid_textrank\n",
    "rapid = BaseTextRank(top_n=10, language=\"en\")\n",
    "rapid_result = rapid.extract_keywords(text)\n",
    "rapid_phrases = [(p.text, p.score) for p in rapid_result.phrases]\n",
    "\n",
    "# pytextrank\n",
    "nlp_quality = spacy.load(\"en_core_web_sm\")\n",
    "nlp_quality.add_pipe(\"textrank\")\n",
    "doc = nlp_quality(text)\n",
    "py_phrases = [(p.text, p.rank) for p in doc._.phrases[:10]]\n",
    "\n",
    "# Display side by side\n",
    "print(f\"{'rapid_textrank':<40} {'pytextrank':<40}\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(10):\n",
    "    rapid_item = f\"{i+1}. {rapid_phrases[i][0]}\" if i < len(rapid_phrases) else \"\"\n",
    "    py_item = f\"{i+1}. {py_phrases[i][0]}\" if i < len(py_phrases) else \"\"\n",
    "    print(f\"{rapid_item:<40} {py_item:<40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overlap-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overlap\n",
    "rapid_set = {p[0].lower() for p in rapid_phrases}\n",
    "py_set = {p[0].lower() for p in py_phrases}\n",
    "\n",
    "overlap = rapid_set & py_set\n",
    "only_rapid = rapid_set - py_set\n",
    "only_py = py_set - rapid_set\n",
    "\n",
    "print(\"Overlap Analysis:\")\n",
    "print(f\"  Common phrases: {len(overlap)}\")\n",
    "print(f\"  Only in rapid_textrank: {len(only_rapid)}\")\n",
    "print(f\"  Only in pytextrank: {len(only_py)}\")\n",
    "print(f\"\\nJaccard similarity: {len(overlap) / len(rapid_set | py_set):.2%}\")\n",
    "print(f\"\\nCommon: {overlap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "| Benchmark | Speedup |\n",
    "|-----------|--------:|\n",
    "| End-to-end (small text) | ~15-30x |\n",
    "| End-to-end (large text) | ~10-20x |\n",
    "| Extraction-only | ~5-15x |\n",
    "| Batch processing | ~10-30x |\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- **Use rapid_textrank when:**\n",
    "  - Processing large volumes of documents\n",
    "  - You need fast extraction and don't need spaCy's other NLP features\n",
    "  - Building real-time applications\n",
    "\n",
    "- **Use pytextrank when:**\n",
    "  - You're already using spaCy for other NLP tasks\n",
    "  - You need pytextrank-specific features like summarization\n",
    "  - Speed is not a primary concern\n",
    "\n",
    "- **Use rapid_textrank's spaCy component when:**\n",
    "  - You want the best of both worlds: spaCy's tokenization + rapid_textrank's speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = {\n",
    "    \"Text Size\": [],\n",
    "    \"rapid_textrank (ms)\": [],\n",
    "    \"pytextrank (ms)\": [],\n",
    "    \"Speedup\": []\n",
    "}\n",
    "\n",
    "for size in TEXTS.keys():\n",
    "    summary_data[\"Text Size\"].append(size.capitalize())\n",
    "    summary_data[\"rapid_textrank (ms)\"].append(f\"{results[size]['rapid_textrank']['mean']:.2f}\")\n",
    "    summary_data[\"pytextrank (ms)\"].append(f\"{results[size]['pytextrank']['mean']:.2f}\")\n",
    "    summary_data[\"Speedup\"].append(f\"{results[size]['speedup']:.1f}x\")\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\nPerformance Summary (End-to-End):\")\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}