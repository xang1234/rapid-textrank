{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Algorithm Variants in rapid_textrank\n",
    "\n",
    "This notebook explores the TextRank algorithm variants available in `rapid_textrank`:\n",
    "\n",
    "| Variant | Best For | Key Feature |\n",
    "|---------|----------|-------------|\n",
    "| **BaseTextRank** | General text | Standard TextRank implementation |\n",
    "| **PositionRank** | Academic papers, news | Favors words appearing early |\n",
    "| **BiasedTextRank** | Topic-focused extraction | Biases toward specified focus terms |\n",
    "| **TopicalPageRank** | Domain-specific extraction | Biases toward topic-weighted terms via personalized PageRank |\n",
    "| **MultipartiteRank** | Multi-topic documents | K-partite graph with intra-topic edges removed; boosts first-occurring variants |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install if needed\n",
    "%pip install -q rapid_textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapid_textrank import BaseTextRank, PositionRank, BiasedTextRank, TopicalPageRank, MultipartiteRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basetextrank-header",
   "metadata": {},
   "source": [
    "## 1. BaseTextRank\n",
    "\n",
    "The standard TextRank algorithm, based on [Mihalcea & Tarau (2004)](https://aclanthology.org/W04-3252/).\n",
    "\n",
    "**How it works:**\n",
    "1. Builds a co-occurrence graph from content words\n",
    "2. Runs PageRank to score word importance\n",
    "3. Extracts phrases by grouping high-scoring words\n",
    "\n",
    "**Best for:** General-purpose keyword extraction where word position doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "basetextrank-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextRank Results:\n",
      "==================================================\n",
      " 1. generate human language             0.1374\n",
      " 2. NLP tasks                           0.1308\n",
      " 3. NLP                                 0.0970\n",
      " 4. Natural language                    0.0745\n",
      " 5. enable computers                    0.0642\n",
      " 6. humans                              0.0475\n",
      " 7. understand interpret                0.0453\n",
      " 8. artificial intelligence             0.0443\n",
      " 9. computers                           0.0396\n",
      "10. models                              0.0357\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence\n",
    "that focuses on the interaction between computers and humans through\n",
    "natural language. The ultimate goal of NLP is to enable computers to\n",
    "understand, interpret, and generate human language in a valuable way.\n",
    "\n",
    "Machine learning approaches have transformed NLP in recent years.\n",
    "Deep learning models, particularly transformers, have achieved\n",
    "state-of-the-art results on many NLP tasks including translation,\n",
    "summarization, and question answering.\n",
    "\"\"\"\n",
    "\n",
    "base = BaseTextRank(top_n=10, language=\"en\")\n",
    "result = base.extract_keywords(text)\n",
    "\n",
    "print(\"BaseTextRank Results:\")\n",
    "print(\"=\" * 50)\n",
    "for p in result.phrases:\n",
    "    print(f\"{p.rank:>2}. {p.text:<35} {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positionrank-header",
   "metadata": {},
   "source": [
    "## 2. PositionRank\n",
    "\n",
    "Based on [Florescu & Caragea (2017)](https://aclanthology.org/P17-1102/), PositionRank weights words by their position in the document.\n",
    "\n",
    "**Key insight:** In many documents (papers, news articles, reports), important terms appear early—in titles, abstracts, or introductory paragraphs.\n",
    "\n",
    "**How it differs from BaseTextRank:**\n",
    "- Words appearing early get higher initial importance\n",
    "- Position weight decays as you move through the document\n",
    "- The PageRank algorithm then refines these position-biased scores\n",
    "\n",
    "**Best for:** Academic papers, news articles, structured documents with front-loaded information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positionrank-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionRank Results:\n",
      "==================================================\n",
      " 1. Quantum Error Correction            0.4726\n",
      " 2. Term Quantum Computers              0.4123\n",
      " 3. fault tolerant quantum computation  0.0690\n",
      " 4. logical error rates                 0.0425\n",
      " 5. practical quantum                   0.0377\n",
      " 6. correct errors                      0.0362\n",
      " 7. method leverages machine            0.0289\n",
      " 8. real time                           0.0158\n",
      " 9. qubits demonstrate                  0.0142\n",
      "10. overhead                            0.0128\n"
     ]
    }
   ],
   "source": [
    "# Academic abstract where key terms appear in the title/first sentence\n",
    "abstract = \"\"\"\n",
    "Quantum Error Correction in Near-Term Quantum Computers\n",
    "\n",
    "We present a novel approach to quantum error correction that significantly\n",
    "reduces the overhead required for fault-tolerant quantum computation.\n",
    "Our method leverages machine learning to predict and correct errors\n",
    "in real-time. Experimental results on superconducting qubits demonstrate\n",
    "a 50% reduction in logical error rates. These advances bring us closer\n",
    "to practical quantum computing applications.\n",
    "\"\"\"\n",
    "\n",
    "pos = PositionRank(top_n=10, language=\"en\")\n",
    "result = pos.extract_keywords(abstract)\n",
    "\n",
    "print(\"PositionRank Results:\")\n",
    "print(\"=\" * 50)\n",
    "for p in result.phrases:\n",
    "    print(f\"{p.rank:>2}. {p.text:<35} {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "### BaseTextRank vs PositionRank: Side-by-Side\n",
    "\n",
    "Let's compare both algorithms on the same text to see how position weighting affects results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextRank                        PositionRank                       \n",
      "======================================================================\n",
      "fault tolerant quantum computation  Quantum Error Correction           \n",
      "quantum error correction            Term Quantum Computers             \n",
      "logical error rates                 fault tolerant quantum computation \n",
      "practical quantum                   logical error rates                \n",
      "correct errors                      practical quantum                  \n"
     ]
    }
   ],
   "source": [
    "# Same abstract, both algorithms\n",
    "base = BaseTextRank(top_n=5, language=\"en\")\n",
    "pos = PositionRank(top_n=5, language=\"en\")\n",
    "\n",
    "base_result = base.extract_keywords(abstract)\n",
    "pos_result = pos.extract_keywords(abstract)\n",
    "\n",
    "print(f\"{'BaseTextRank':<35} {'PositionRank':<35}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(5):\n",
    "    base_phrase = base_result.phrases[i]\n",
    "    pos_phrase = pos_result.phrases[i]\n",
    "    print(f\"{base_phrase.text:<35} {pos_phrase.text:<35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comparison-detailed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In both: {'fault tolerant quantum computation', 'practical quantum', 'logical error rates'}\n",
      "Only in BaseTextRank: {'quantum error correction', 'correct errors'}\n",
      "Only in PositionRank: {'Quantum Error Correction', 'Term Quantum Computers'}\n"
     ]
    }
   ],
   "source": [
    "# Let's see which phrases are unique to each algorithm\n",
    "base_texts = {p.text for p in base_result.phrases}\n",
    "pos_texts = {p.text for p in pos_result.phrases}\n",
    "\n",
    "only_base = base_texts - pos_texts\n",
    "only_pos = pos_texts - base_texts\n",
    "both = base_texts & pos_texts\n",
    "\n",
    "print(f\"In both: {both}\")\n",
    "print(f\"Only in BaseTextRank: {only_base}\")\n",
    "print(f\"Only in PositionRank: {only_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biasedtextrank-header",
   "metadata": {},
   "source": [
    "## 3. BiasedTextRank\n",
    "\n",
    "Based on [Kazemi et al. (2020)](https://aclanthology.org/2020.coling-main.144/), BiasedTextRank steers extraction toward specified focus terms.\n",
    "\n",
    "**Key parameters:**\n",
    "- `focus_terms`: List of terms to bias toward\n",
    "- `bias_weight`: How strongly to favor focus terms (higher = stronger bias)\n",
    "\n",
    "**How it works:**\n",
    "- Focus terms get an initial boost in the PageRank algorithm\n",
    "- Words connected to focus terms inherit some of this bias\n",
    "- The result emphasizes the topic you care about\n",
    "\n",
    "**Best for:** Topic-specific extraction, document filtering, aspect-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "biasedtextrank-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTextRank (focus: security, privacy):\n",
      "==================================================\n",
      " 1. balance user experience             0.1326\n",
      " 2. sensitive user                      0.1018\n",
      " 3. mobile users                        0.1018\n",
      " 4. complicate data freshness           0.0947\n",
      " 5. Encryption protects data            0.0912\n",
      " 6. careful data                        0.0872\n",
      " 7. strategies improve response times   0.0832\n",
      " 8. user                                0.0751\n",
      " 9. GDPR require                        0.0649\n",
      "10. Security vulnerabilities            0.0638\n"
     ]
    }
   ],
   "source": [
    "# Text covering multiple topics\n",
    "tech_article = \"\"\"\n",
    "Modern web applications must balance user experience with security.\n",
    "Performance optimizations are crucial for mobile users on slow networks.\n",
    "Privacy regulations like GDPR require careful data handling and consent.\n",
    "Security vulnerabilities can expose sensitive user information.\n",
    "Caching strategies improve response times but complicate data freshness.\n",
    "Authentication systems must prevent unauthorized access while remaining\n",
    "user-friendly. Encryption protects data both in transit and at rest.\n",
    "\"\"\"\n",
    "\n",
    "# Focus on security/privacy topics\n",
    "biased = BiasedTextRank(\n",
    "    focus_terms=[\"security\", \"privacy\"],\n",
    "    bias_weight=5.0,\n",
    "    top_n=10,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "result = biased.extract_keywords(tech_article)\n",
    "\n",
    "print(\"BiasedTextRank (focus: security, privacy):\")\n",
    "print(\"=\" * 50)\n",
    "for p in result.phrases:\n",
    "    print(f\"{p.rank:>2}. {p.text:<35} {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bias-weight-header",
   "metadata": {},
   "source": [
    "### Effect of `bias_weight`\n",
    "\n",
    "The `bias_weight` parameter controls how strongly results favor the focus terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bias-weight-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Weight Comparison (focus: security, privacy)\n",
      "================================================================================\n",
      "\n",
      "bias_weight=1.0:\n",
      "  balance user experience: 0.1158\n",
      "  complicate data freshness: 0.1123\n",
      "  Encryption protects data: 0.1077\n",
      "\n",
      "bias_weight=2.0:\n",
      "  balance user experience: 0.1209\n",
      "  complicate data freshness: 0.1070\n",
      "  Encryption protects data: 0.1027\n",
      "\n",
      "bias_weight=5.0:\n",
      "  balance user experience: 0.1326\n",
      "  sensitive user: 0.1018\n",
      "  mobile users: 0.1018\n",
      "\n",
      "bias_weight=10.0:\n",
      "  balance user experience: 0.1453\n",
      "  sensitive user: 0.1127\n",
      "  mobile users: 0.1082\n"
     ]
    }
   ],
   "source": [
    "# Compare different bias weights\n",
    "weights = [1.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "print(\"Bias Weight Comparison (focus: security, privacy)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for weight in weights:\n",
    "    biased = BiasedTextRank(\n",
    "        focus_terms=[\"security\", \"privacy\"],\n",
    "        bias_weight=weight,\n",
    "        top_n=3,\n",
    "        language=\"en\"\n",
    "    )\n",
    "    result = biased.extract_keywords(tech_article)\n",
    "    \n",
    "    print(f\"\\nbias_weight={weight}:\")\n",
    "    for p in result.phrases:\n",
    "        print(f\"  {p.text}: {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bias-vs-unbiased",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased                  Security Focus            Performance Focus        \n",
      "===========================================================================\n",
      "balance user experience   balance user experience   balance user experience  \n",
      "complicate data freshness sensitive user            mobile users             \n",
      "Encryption protects data  mobile users              complicate data freshness\n",
      "strategies improve response times complicate data freshness Encryption protects data \n",
      "careful data              Encryption protects data  strategies improve response times\n"
     ]
    }
   ],
   "source": [
    "# Compare biased vs unbiased on the same text\n",
    "base = BaseTextRank(top_n=5, language=\"en\")\n",
    "biased_security = BiasedTextRank(\n",
    "    focus_terms=[\"security\", \"privacy\"],\n",
    "    bias_weight=5.0,\n",
    "    top_n=5,\n",
    "    language=\"en\"\n",
    ")\n",
    "biased_perf = BiasedTextRank(\n",
    "    focus_terms=[\"performance\", \"speed\", \"cache\"],\n",
    "    bias_weight=5.0,\n",
    "    top_n=5,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "base_result = base.extract_keywords(tech_article)\n",
    "security_result = biased_security.extract_keywords(tech_article)\n",
    "perf_result = biased_perf.extract_keywords(tech_article)\n",
    "\n",
    "print(f\"{'Unbiased':<25} {'Security Focus':<25} {'Performance Focus':<25}\")\n",
    "print(\"=\" * 75)\n",
    "for i in range(5):\n",
    "    print(f\"{base_result.phrases[i].text:<25} \"\n",
    "          f\"{security_result.phrases[i].text:<25} \"\n",
    "          f\"{perf_result.phrases[i].text:<25}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focus-per-call",
   "metadata": {},
   "source": [
    "### Dynamic Focus Terms\n",
    "\n",
    "You can also pass `focus_terms` per-call, which is useful when processing multiple documents with different topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "per-call-focus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Focus: ['security', 'encryption']\n",
      "  - balance user experience\n",
      "  - Encryption protects data\n",
      "  - complicate data freshness\n",
      "\n",
      "Focus: ['performance', 'caching']\n",
      "  - balance user experience\n",
      "  - mobile users\n",
      "  - complicate data freshness\n",
      "\n",
      "Focus: ['user', 'experience']\n",
      "  - balance user experience\n",
      "  - mobile users\n",
      "  - sensitive user\n"
     ]
    }
   ],
   "source": [
    "# Create extractor with default focus\n",
    "extractor = BiasedTextRank(\n",
    "    focus_terms=[\"default\"],  # Placeholder\n",
    "    bias_weight=5.0,\n",
    "    top_n=5,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "# Override focus_terms per call\n",
    "topics = [\n",
    "    [\"security\", \"encryption\"],\n",
    "    [\"performance\", \"caching\"],\n",
    "    [\"user\", \"experience\"]\n",
    "]\n",
    "\n",
    "for focus in topics:\n",
    "    result = extractor.extract_keywords(tech_article, focus_terms=focus)\n",
    "    print(f\"\\nFocus: {focus}\")\n",
    "    for p in result.phrases[:3]:\n",
    "        print(f\"  - {p.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pklx73y7cil",
   "metadata": {},
   "source": [
    "## 4. TopicalPageRank\n",
    "\n",
    "Based on [Sterckx et al. (2015)](https://aclanthology.org/), TopicalPageRank uses **personalized PageRank** to steer extraction toward topic-relevant terms.\n",
    "\n",
    "**Key parameters:**\n",
    "- `topic_weights`: Dict mapping lemmas to importance weights (e.g. from LDA topics)\n",
    "- `min_weight`: Baseline weight for words not in the topic vocabulary (default `0.0`)\n",
    "\n",
    "**How it differs from BiasedTextRank:**\n",
    "- BiasedTextRank boosts *specific focus terms* you provide (e.g. \"security\", \"privacy\")\n",
    "- TopicalPageRank uses a *distribution of weights* over many terms, typically derived from topic models like LDA\n",
    "- The weights act as the PageRank teleport distribution — when the random surfer restarts, it jumps to nodes proportionally to their topic weight\n",
    "- Only relative proportions matter; weights are normalized internally\n",
    "\n",
    "**Best for:** Domain-specific extraction where you have topic model output or domain vocabularies with graded importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bkdza81hrdw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicalPageRank (security/privacy topic weights):\n",
      "==================================================\n",
      " 1. Encryption protects data            0.1347\n",
      " 2. balance user experience             0.1105\n",
      " 3. complicate data freshness           0.1069\n",
      " 4. mobile users                        0.0922\n",
      " 5. sensitive user                      0.0918\n",
      " 6. careful data                        0.0911\n",
      " 7. Security vulnerabilities            0.0759\n",
      " 8. user                                0.0750\n",
      " 9. Privacy regulations                 0.0694\n",
      "10. Authentication systems              0.0606\n"
     ]
    }
   ],
   "source": [
    "# Text covering multiple topics — same one used for BiasedTextRank above\n",
    "# TopicalPageRank lets us weight many terms at once with graded importance\n",
    "\n",
    "topic_weights = {\n",
    "    \"security\": 0.9,\n",
    "    \"privacy\": 0.8,\n",
    "    \"encryption\": 0.7,\n",
    "    \"authentication\": 0.6,\n",
    "    \"data\": 0.4,\n",
    "    \"access\": 0.3,\n",
    "}\n",
    "\n",
    "tpr = TopicalPageRank(\n",
    "    topic_weights=topic_weights,\n",
    "    min_weight=0.0,   # OOV words get zero teleport probability\n",
    "    top_n=10,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "result = tpr.extract_keywords(tech_article)\n",
    "\n",
    "print(\"TopicalPageRank (security/privacy topic weights):\")\n",
    "print(\"=\" * 50)\n",
    "for p in result.phrases:\n",
    "    print(f\"{p.rank:>2}. {p.text:<35} {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aprveq11sb",
   "metadata": {},
   "source": [
    "### Effect of `min_weight`\n",
    "\n",
    "The `min_weight` parameter controls how much \"attention\" out-of-vocabulary words receive during the random walk's teleport step:\n",
    "\n",
    "- `min_weight=0.0` — Only topic-relevant words get teleport probability (strong focus)\n",
    "- `min_weight > 0` — All words get at least this baseline, softening the topic bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "j2tah087xh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_weight Comparison (same topic_weights)\n",
      "================================================================================\n",
      "\n",
      "min_weight=0.0:\n",
      "  Encryption protects data: 0.1347\n",
      "  balance user experience: 0.1105\n",
      "  complicate data freshness: 0.1069\n",
      "\n",
      "min_weight=0.01:\n",
      "  Encryption protects data: 0.1321\n",
      "  balance user experience: 0.1120\n",
      "  complicate data freshness: 0.1066\n",
      "\n",
      "min_weight=0.1:\n",
      "  balance user experience: 0.1198\n",
      "  Encryption protects data: 0.1189\n",
      "  complicate data freshness: 0.1055\n",
      "\n",
      "min_weight=0.5:\n",
      "  balance user experience: 0.1278\n",
      "  Encryption protects data: 0.1054\n",
      "  complicate data freshness: 0.1044\n"
     ]
    }
   ],
   "source": [
    "# Compare different min_weight values\n",
    "min_weights = [0.0, 0.01, 0.1, 0.5]\n",
    "\n",
    "print(\"min_weight Comparison (same topic_weights)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for mw in min_weights:\n",
    "    tpr = TopicalPageRank(\n",
    "        topic_weights=topic_weights,\n",
    "        min_weight=mw,\n",
    "        top_n=3,\n",
    "        language=\"en\"\n",
    "    )\n",
    "    result = tpr.extract_keywords(tech_article)\n",
    "\n",
    "    print(f\"\\nmin_weight={mw}:\")\n",
    "    for p in result.phrases:\n",
    "        print(f\"  {p.text}: {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y4stm10d4is",
   "metadata": {},
   "source": [
    "### TopicalPageRank vs BiasedTextRank: Side-by-Side\n",
    "\n",
    "Both variants let you steer extraction toward a topic, but they work differently:\n",
    "- **BiasedTextRank** takes a flat list of focus terms with a single `bias_weight` multiplier\n",
    "- **TopicalPageRank** takes a *weighted vocabulary* and uses personalized PageRank teleportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3hwu49xopbm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTextRank                      TopicalPageRank                    \n",
      "======================================================================\n",
      "balance user experience             Encryption protects data           \n",
      "Encryption protects data            balance user experience            \n",
      "complicate data freshness           complicate data freshness          \n",
      "careful data                        mobile users                       \n",
      "sensitive user                      sensitive user                     \n"
     ]
    }
   ],
   "source": [
    "# Compare BiasedTextRank and TopicalPageRank on the same security focus\n",
    "biased = BiasedTextRank(\n",
    "    focus_terms=[\"security\", \"privacy\", \"encryption\", \"authentication\"],\n",
    "    bias_weight=5.0,\n",
    "    top_n=5,\n",
    "    language=\"en\"\n",
    ")\n",
    "tpr = TopicalPageRank(\n",
    "    topic_weights=topic_weights,\n",
    "    min_weight=0.0,\n",
    "    top_n=5,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "biased_result = biased.extract_keywords(tech_article)\n",
    "tpr_result = tpr.extract_keywords(tech_article)\n",
    "\n",
    "print(f\"{'BiasedTextRank':<35} {'TopicalPageRank':<35}\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    b = biased_result.phrases[i].text if i < len(biased_result.phrases) else \"\"\n",
    "    t = tpr_result.phrases[i].text if i < len(tpr_result.phrases) else \"\"\n",
    "    print(f\"{b:<35} {t:<35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me563wag5go",
   "metadata": {},
   "source": [
    "## 5. MultipartiteRank\n",
    "\n",
    "Based on [Boudin (2018)](https://aclanthology.org/N18-2105/), MultipartiteRank extends TopicRank by keeping individual candidates as graph nodes instead of collapsing topics.\n",
    "\n",
    "**Key parameters:**\n",
    "- `similarity_threshold`: Jaccard similarity threshold for clustering candidates into topics (default `0.26`)\n",
    "- `alpha`: Position boost strength — higher values give more weight to first-occurring variants (default `1.1`, `0` disables)\n",
    "\n",
    "**How it works:**\n",
    "1. Candidates are clustered into topics (same HAC clustering as TopicRank)\n",
    "2. A k-partite graph is built: edges connect candidates from **different** topics only\n",
    "3. Edge weights are inversely proportional to the positional gap between candidates\n",
    "4. An alpha adjustment **boosts incoming edges** to the first-occurring variant in each topic\n",
    "5. PageRank is run on this modified graph\n",
    "\n",
    "**How it differs from TopicRank:**\n",
    "- **TopicRank** collapses each topic into a single node and ranks topics as a whole\n",
    "- **MultipartiteRank** keeps every candidate as its own node but removes intra-topic edges, preserving fine-grained distinctions while preventing intra-topic competition\n",
    "\n",
    "**Best for:** Multi-topic documents where you want topic-aware ranking with positional preference for earlier mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sqeohr48vm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipartiteRank Results:\n",
      "==================================================\n",
      " 1. NLP                                 0.1416\n",
      " 2. Natural language                    0.1216\n",
      " 3. computers                           0.1012\n",
      " 4. field                               0.0447\n",
      " 5. artificial intelligence             0.0443\n",
      " 6. interaction                         0.0415\n",
      " 7. humans                              0.0411\n",
      " 8. focuses                             0.0393\n",
      " 9. ultimate                            0.0311\n",
      "10. translation                         0.0277\n"
     ]
    }
   ],
   "source": [
    "# MultipartiteRank on the same NLP text\n",
    "mpr = MultipartiteRank(\n",
    "    similarity_threshold=0.26,\n",
    "    alpha=1.1,\n",
    "    top_n=10,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "result = mpr.extract_keywords(text)\n",
    "\n",
    "print(\"MultipartiteRank Results:\")\n",
    "print(\"=\" * 50)\n",
    "for p in result.phrases:\n",
    "    print(f\"{p.rank:>2}. {p.text:<35} {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t58fv2blua",
   "metadata": {},
   "source": [
    "### Effect of `alpha`\n",
    "\n",
    "The `alpha` parameter controls the position boost for first-occurring variants in each topic cluster. Setting `alpha=0` disables the boost entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0tl94094vcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Comparison (similarity_threshold=0.26)\n",
      "================================================================================\n",
      "\n",
      "alpha=0.0:\n",
      "  translation: 0.0477\n",
      "  summarization: 0.0470\n",
      "  natural language: 0.0463\n",
      "\n",
      "alpha=0.5:\n",
      "  NLP: 0.0942\n",
      "  Natural language: 0.0842\n",
      "  computers: 0.0633\n",
      "\n",
      "alpha=1.1:\n",
      "  NLP: 0.1416\n",
      "  Natural language: 0.1216\n",
      "  computers: 0.1012\n",
      "\n",
      "alpha=2.0:\n",
      "  NLP: 0.1823\n",
      "  computers: 0.1589\n",
      "  Natural language: 0.1526\n"
     ]
    }
   ],
   "source": [
    "# Compare different alpha values\n",
    "alphas = [0.0, 0.5, 1.1, 2.0]\n",
    "\n",
    "print(\"Alpha Comparison (similarity_threshold=0.26)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for alpha in alphas:\n",
    "    mpr = MultipartiteRank(\n",
    "        similarity_threshold=0.26,\n",
    "        alpha=alpha,\n",
    "        top_n=3,\n",
    "        language=\"en\"\n",
    "    )\n",
    "    result = mpr.extract_keywords(text)\n",
    "\n",
    "    print(f\"\\nalpha={alpha}:\")\n",
    "    for p in result.phrases:\n",
    "        print(f\"  {p.text}: {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49mx0h2u5j7",
   "metadata": {},
   "source": [
    "### MultipartiteRank vs BaseTextRank: Side-by-Side\n",
    "\n",
    "Let's compare MultipartiteRank with BaseTextRank to see how topic-aware graph construction affects results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62npjpi999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextRank                        MultipartiteRank                   \n",
      "======================================================================\n",
      "generate human language             NLP                                \n",
      "NLP tasks                           Natural language                   \n",
      "NLP                                 computers                          \n",
      "Natural language                    field                              \n",
      "enable computers                    artificial intelligence            \n"
     ]
    }
   ],
   "source": [
    "# Compare BaseTextRank and MultipartiteRank on the same text\n",
    "base = BaseTextRank(top_n=5, language=\"en\")\n",
    "mpr = MultipartiteRank(similarity_threshold=0.26, alpha=1.1, top_n=5, language=\"en\")\n",
    "\n",
    "base_result = base.extract_keywords(text)\n",
    "mpr_result = mpr.extract_keywords(text)\n",
    "\n",
    "print(f\"{'BaseTextRank':<35} {'MultipartiteRank':<35}\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    b = base_result.phrases[i].text if i < len(base_result.phrases) else \"\"\n",
    "    m = mpr_result.phrases[i].text if i < len(mpr_result.phrases) else \"\"\n",
    "    print(f\"{b:<35} {m:<35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json-header",
   "metadata": {},
   "source": [
    "## JSON Batch API\n",
    "\n",
    "For processing large volumes of pre-tokenized documents, use the JSON interface:\n",
    "\n",
    "- `extract_from_json()` - Single document\n",
    "- `extract_batch_from_json()` - Multiple documents\n",
    "\n",
    "This is particularly useful when:\n",
    "- You've already tokenized with spaCy or another NLP library\n",
    "- You're processing many documents in batch\n",
    "- You need fine-grained control over tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "json-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single document result:\n",
      "  Machine learning: 0.5000\n",
      "  industries: 0.2048\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rapid_textrank import extract_from_json\n",
    "\n",
    "# Single document with pre-tokenized input\n",
    "doc = {\n",
    "    \"tokens\": [\n",
    "        {\"text\": \"Machine\", \"lemma\": \"machine\", \"pos\": \"NOUN\",\n",
    "         \"start\": 0, \"end\": 7, \"sentence_idx\": 0, \"token_idx\": 0, \"is_stopword\": False},\n",
    "        {\"text\": \"learning\", \"lemma\": \"learning\", \"pos\": \"NOUN\",\n",
    "         \"start\": 8, \"end\": 16, \"sentence_idx\": 0, \"token_idx\": 1, \"is_stopword\": False},\n",
    "        {\"text\": \"is\", \"lemma\": \"be\", \"pos\": \"AUX\",\n",
    "         \"start\": 17, \"end\": 19, \"sentence_idx\": 0, \"token_idx\": 2, \"is_stopword\": True},\n",
    "        {\"text\": \"transforming\", \"lemma\": \"transform\", \"pos\": \"VERB\",\n",
    "         \"start\": 20, \"end\": 32, \"sentence_idx\": 0, \"token_idx\": 3, \"is_stopword\": False},\n",
    "        {\"text\": \"industries\", \"lemma\": \"industry\", \"pos\": \"NOUN\",\n",
    "         \"start\": 33, \"end\": 43, \"sentence_idx\": 0, \"token_idx\": 4, \"is_stopword\": False},\n",
    "    ],\n",
    "    \"config\": {\n",
    "        \"top_n\": 5,\n",
    "        \"window_size\": 3,\n",
    "        \"damping\": 0.85\n",
    "    }\n",
    "}\n",
    "\n",
    "result_json = extract_from_json(json.dumps(doc))\n",
    "result = json.loads(result_json)\n",
    "\n",
    "print(\"Single document result:\")\n",
    "for phrase in result[\"phrases\"]:\n",
    "    print(f\"  {phrase['text']}: {phrase['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "json-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch results:\n",
      "\n",
      "Document 0:\n",
      "  - Deep learning models (1.0000)\n",
      "\n",
      "Document 1:\n",
      "  - Neural networks (0.5000)\n",
      "  - data (0.2048)\n"
     ]
    }
   ],
   "source": [
    "from rapid_textrank import extract_batch_from_json\n",
    "\n",
    "# Batch processing multiple documents\n",
    "docs = [\n",
    "    {\n",
    "        \"tokens\": [\n",
    "            {\"text\": \"Deep\", \"lemma\": \"deep\", \"pos\": \"ADJ\",\n",
    "             \"start\": 0, \"end\": 4, \"sentence_idx\": 0, \"token_idx\": 0, \"is_stopword\": False},\n",
    "            {\"text\": \"learning\", \"lemma\": \"learning\", \"pos\": \"NOUN\",\n",
    "             \"start\": 5, \"end\": 13, \"sentence_idx\": 0, \"token_idx\": 1, \"is_stopword\": False},\n",
    "            {\"text\": \"models\", \"lemma\": \"model\", \"pos\": \"NOUN\",\n",
    "             \"start\": 14, \"end\": 20, \"sentence_idx\": 0, \"token_idx\": 2, \"is_stopword\": False},\n",
    "        ],\n",
    "        \"config\": {\"top_n\": 3}\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\n",
    "            {\"text\": \"Neural\", \"lemma\": \"neural\", \"pos\": \"ADJ\",\n",
    "             \"start\": 0, \"end\": 6, \"sentence_idx\": 0, \"token_idx\": 0, \"is_stopword\": False},\n",
    "            {\"text\": \"networks\", \"lemma\": \"network\", \"pos\": \"NOUN\",\n",
    "             \"start\": 7, \"end\": 15, \"sentence_idx\": 0, \"token_idx\": 1, \"is_stopword\": False},\n",
    "            {\"text\": \"process\", \"lemma\": \"process\", \"pos\": \"VERB\",\n",
    "             \"start\": 16, \"end\": 23, \"sentence_idx\": 0, \"token_idx\": 2, \"is_stopword\": False},\n",
    "            {\"text\": \"data\", \"lemma\": \"data\", \"pos\": \"NOUN\",\n",
    "             \"start\": 24, \"end\": 28, \"sentence_idx\": 0, \"token_idx\": 3, \"is_stopword\": False},\n",
    "        ],\n",
    "        \"config\": {\"top_n\": 3}\n",
    "    }\n",
    "]\n",
    "\n",
    "results_json = extract_batch_from_json(json.dumps(docs))\n",
    "results = json.loads(results_json)\n",
    "\n",
    "print(\"Batch results:\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    for phrase in result[\"phrases\"]:\n",
    "        print(f\"  - {phrase['text']} ({phrase['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision-guide",
   "metadata": {},
   "source": [
    "## Decision Guide: Which Variant to Use?\n",
    "\n",
    "```\n",
    "                                START\n",
    "                                  │\n",
    "                                  ▼\n",
    "                    ┌─────────────────────────┐\n",
    "                    │ Do you have specific    │\n",
    "                    │ topics to focus on?     │\n",
    "                    └─────────────────────────┘\n",
    "                         │              │\n",
    "                        YES             NO\n",
    "                         │              │\n",
    "                         ▼              ▼\n",
    "              ┌────────────────────┐  ┌─────────────────────────┐\n",
    "              │ Do you have graded │  │ Is key info at the     │\n",
    "              │ topic weights      │  │ beginning of the doc?   │\n",
    "              │ (e.g. from LDA)?   │  └─────────────────────────┘\n",
    "              └────────────────────┘       │              │\n",
    "                   │            │         YES             NO\n",
    "                  YES           NO         │              │\n",
    "                   │            │          ▼              ▼\n",
    "                   ▼            ▼  ┌──────────────┐ ┌──────────────────────────┐\n",
    "          ┌────────────────┐ ┌──────────────┐      │ PositionRank │ │ Multiple topics with     │\n",
    "          │TopicalPageRank │ │BiasedTextRank│      └──────────────┘ │ repeated candidates?     │\n",
    "          └────────────────┘ └──────────────┘                       └──────────────────────────┘\n",
    "                                                                         │              │\n",
    "                                                                        YES             NO\n",
    "                                                                         │              │\n",
    "                                                                         ▼              ▼\n",
    "                                                                 ┌──────────────────┐ ┌──────────────┐\n",
    "                                                                 │MultipartiteRank  │ │ BaseTextRank │\n",
    "                                                                 └──────────────────┘ └──────────────┘\n",
    "```\n",
    "\n",
    "### Recommendations by Document Type\n",
    "\n",
    "| Document Type | Recommended Variant | Why |\n",
    "|---------------|---------------------|-----|\n",
    "| Blog posts, articles | BaseTextRank | General content, no position bias needed |\n",
    "| Academic papers | PositionRank | Key terms in title/abstract |\n",
    "| News articles | PositionRank | Lead paragraphs contain key info |\n",
    "| Product reviews | BiasedTextRank | Focus on features you care about |\n",
    "| Support tickets | BiasedTextRank | Focus on problem categories |\n",
    "| Legal documents | BaseTextRank | Important terms throughout |\n",
    "| Domain corpora with LDA | TopicalPageRank | Graded topic weights from topic models |\n",
    "| Taxonomy-guided extraction | TopicalPageRank | Weight terms by domain vocabulary importance |\n",
    "| Multi-topic documents | MultipartiteRank | Topic-aware with positional preference |\n",
    "| Documents with repeated terminology | MultipartiteRank | Deduplicates via topic clustering, boosts first mention |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[03_explain_algorithm.ipynb](03_explain_algorithm.ipynb)** - Visual explanation of how TextRank works internally\n",
    "- **[04_benchmarks.ipynb](04_benchmarks.ipynb)** - Performance comparison with pytextrank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
